{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7ef2ab-eaa2-4fd9-b4a5-536662335c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hugging-face-venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'swifter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mswifter\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'swifter'"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras import Model, Input\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from accelerate import Accelerator\n",
    "import swifter\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036e8d72-d4e4-49ea-8cb9-55869cac7fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a20b8b-6cd9-42d9-8248-ea1c98067e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "549c06e9-958c-44cd-9bd8-040adb5c5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='fast',\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "153079e6-7bfe-4a27-804e-91d1ef94bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(small_train_dataset, batch_size=training_args.per_device_train_batch_size)\n",
    "\n",
    "# if training_args.gradient_checkpointing:\n",
    "#     model.gradient_checkpointing_enable()\n",
    "\n",
    "# accelerator = Accelerator()\n",
    "# model, optimizer, dataloader = accelerator.prepare(model, train_dataloader)\n",
    "\n",
    "# model.train()\n",
    "# for step, batch in enumerate(dataloader, start=1):\n",
    "#     loss = model(**batch).loss\n",
    "#     loss = loss / training_args.gradient_accumulation_steps\n",
    "#     accelerator.backward(loss)\n",
    "#     if step % training_args.gradient_accumulation_steps == 0:\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "db22645b-4fc8-417d-a9ce-470739bdae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModel, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95c9dc39-de4c-4af7-be5e-1d7c98929dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.4726463258266449}, {'label': 'LABEL_1', 'score': 0.5273536443710327}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('NEW_MODEL')\n",
    "tokenizer = AutoTokenizer.from_pretrained('NEW_TOKEN')\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "# Now you can use the pipeline\n",
    "result = pipe(\"Your text here\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9d36f1c2-4f7a-4f62-9b1f-7f4925c4c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(text):\n",
    "    pred = pipe(text)\n",
    "    print(pred)\n",
    "    return pred[0][1]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "55bb1fd7-fdd7-479f-8333-80cc2ae1b23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.6134910583496094}, {'label': 'LABEL_1', 'score': 0.386508971452713}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.6508035659790039}, {'label': 'LABEL_1', 'score': 0.3491964042186737}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.6304652690887451}, {'label': 'LABEL_1', 'score': 0.36953476071357727}]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>0.386509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>0.349196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>0.369535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  generated\n",
       "0  0000aaaa   0.386509\n",
       "1  1111bbbb   0.349196\n",
       "2  2222cccc   0.369535"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_path = 'test_essays.csv'\n",
    "test_df = pd.read_csv(final_df_path)\n",
    "test_df['generated'] = test_df['text'].apply(get_pred)\n",
    "test_df = test_df[[\"id\", \"generated\"]]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a9c741d3-1c66-4aa0-98e1-69d0ebef3936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x377617790>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-venv",
   "language": "python",
   "name": "hugging-face-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
