# Finetuning LLMs to classify Human-Written & LLM-Written Essays

## Project Overview

This project aims to classify essays generated by Large Language Models (LLMs) and distinguish them from those written by humans. By leveraging pre-trained models from Hugging Face, I fine-tune these models for the task of essay classification. The project follows a structured workflow to prepare data, clean text, and fine-tune LLMs for accurate classification.

## Project Workflow

### 1. Data Preparation (in `Prepare_Data.ipynb`)
Data is gathered from multiple sources, including both human-written essays and LLM-generated text. The `Prepare_Data.ipynb` notebook outlines the steps to:
- Fetch data from various resources.
- Clean and preprocess the text data to ensure high quality.
- Save the preprocessed data for further use.

### 2. Model Fine-Tuning (in `Final_Script.ipynb`)
The fine-tuning of the LLM is done using a pre-trained model from Hugging Face. If you are running the model on a CPU, use the `Final_Script.ipynb` notebook to:
- Load the cleaned data.
- Fine-tune the selected LLM for essay classification.
- Evaluate model performance.

### 3. Multi-GPU Acceleration (in `acc_script.py`)
For users with access to multiple GPUs, the fine-tuning process can be accelerated using the `acc_script.py` script. This script uses the Hugging Face `Accelerate` library to distribute the training process efficiently across available GPUs.

## Requirements
- Python 3.10.9
- Hugging Face Transformers
- Accelerate (for multi-GPU usage)
- Other dependencies listed in `requirements.txt`

## Usage
1. **Data Preparation:**  
   Run the `Prepare_Data.ipynb` notebook to fetch and preprocess the data.
   
2. **Fine-Tuning the Model (CPU):**  
   Use the `Final_Script.ipynb` notebook to fine-tune the model on your machine.
   
3. **Multi-GPU Fine-Tuning:**  
   If you have access to multiple GPUs, execute the `acc_script.py` script to fine-tune the model using distributed training.
